{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression을 클래스로 구현해봅시다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 함수로 구현한 LogisticRegression\n",
    "\n",
    "### - 아래 코드를 참고하여 LinearRegression_class.py에 클래스로 구현된 Logistic Regression을 완성시켜주세요!  \n",
    "\n",
    "<br/>\n",
    "\n",
    "코드 출처: 박성호님의 머신러닝 강의 https://youtu.be/nhzljkpjjFk, https://github.com/neowizard2018/neowizard/blob/master/MachineLearning/ML_LEC_17_Example1.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape =  (10, 1) , y_train.shape =  (10, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20]).reshape(10,1)   \n",
    "y_train = np.array([0, 0, 0, 0,  0,  0,  1,  1,  1,  1]).reshape(10,1)\n",
    "X_test = np.array([1, 3, 5, 7, 9, 11, 15, 17, 19]).reshape(9,1)\n",
    "\n",
    "print(\"X_train.shape = \", X_train.shape, \", y_train.shape = \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#시그모이드 함수\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "#편미분 함수\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#초기화\n",
    "W = np.random.rand(1,1)  \n",
    "b = np.random.rand(1)  \n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수\n",
    "def loss_func(X_train, y_train):\n",
    "    \n",
    "    delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "    z = np.dot(X_train,W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # cross-entropy \n",
    "    return  -np.sum(y_train*np.log(y + delta) + (1-y_train)*np.log((1 - y)+delta ) )\n",
    "\n",
    "# 손실 값 계산 함수\n",
    "def error_val(X_train, y_train):\n",
    "    \n",
    "    delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "    z = np.dot(X_train,W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # cross-entropy \n",
    "    return  -np.sum( y_train*np.log(y + delta) + (1-y_train)*np.log((1 - y)+delta ) ) \n",
    "\n",
    "\n",
    "def predict(X):\n",
    "    result=[]\n",
    "    for x in X:\n",
    "        z=np.dot(x, W) + b\n",
    "        y=sigmoid(z)\n",
    "\n",
    "        if y > 0.5:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  27.57887674785567 Initial W =  [[0.63383748]] \n",
      " , b =  [0.10596355]\n",
      "step =  0 error value =  11.11645986462114 W =  [[0.22244129]] , b =  [0.05792264]\n",
      "step =  400 error value =  2.8754905059773317 W =  [[0.43174241]] , b =  [-4.2726443]\n",
      "step =  800 error value =  1.7583892053115018 W =  [[0.45985005]] , b =  [-5.72756416]\n",
      "step =  1200 error value =  1.5032650366288312 W =  [[0.53565133]] , b =  [-6.73668177]\n",
      "step =  1600 error value =  1.3425297462549586 W =  [[0.59603574]] , b =  [-7.53839843]\n",
      "step =  2000 error value =  1.2286623197397937 W =  [[0.64698234]] , b =  [-8.2133548]\n",
      "step =  2400 error value =  1.1421094302842594 W =  [[0.69147999]] , b =  [-8.80184179]\n",
      "step =  2800 error value =  1.0731553024641696 W =  [[0.73125343]] , b =  [-9.32708854]\n",
      "step =  3200 error value =  1.016349446029051 W =  [[0.76739556]] , b =  [-9.80379866]\n",
      "step =  3600 error value =  0.9683626151064734 W =  [[0.80064624]] , b =  [-10.24191458]\n",
      "step =  4000 error value =  0.9270287413196204 W =  [[0.83153157]] , b =  [-10.64849862]\n",
      "step =  4400 error value =  0.890868115192958 W =  [[0.86044023]] , b =  [-11.02876259]\n",
      "step =  4800 error value =  0.8588298760024647 W =  [[0.88766826]] , b =  [-11.38667171]\n",
      "step =  5200 error value =  0.8301437109950898 W =  [[0.91344676]] , b =  [-11.7253189]\n",
      "step =  5600 error value =  0.8042298944000387 W =  [[0.93795999]] , b =  [-12.04716727]\n",
      "step =  6000 error value =  0.7806423231934775 W =  [[0.96135737]] , b =  [-12.35421307]\n",
      "step =  6400 error value =  0.7590311218592984 W =  [[0.98376198]] , b =  [-12.64809873]\n",
      "step =  6800 error value =  0.7391173267017166 W =  [[1.00527646]] , b =  [-12.93019327]\n",
      "step =  7200 error value =  0.7206752865857903 W =  [[1.02598742]] , b =  [-13.20165095]\n",
      "step =  7600 error value =  0.7035201414485525 W =  [[1.04596869]] , b =  [-13.46345475]\n",
      "step =  8000 error value =  0.6874987301298502 W =  [[1.06528372]] , b =  [-13.71644937]\n",
      "step =  8400 error value =  0.672482867911485 W =  [[1.08398752]] , b =  [-13.96136649]\n",
      "step =  8800 error value =  0.6583642952688633 W =  [[1.10212813]] , b =  [-14.19884447]\n",
      "step =  9200 error value =  0.6450508269196719 W =  [[1.11974773]] , b =  [-14.42944395]\n",
      "step =  9600 error value =  0.6324633772255095 W =  [[1.13688363]] , b =  [-14.6536602]\n",
      "step =  10000 error value =  0.6205336350133648 W =  [[1.15356896]] , b =  [-14.87193313]\n"
     ]
    }
   ],
   "source": [
    "f = lambda x : loss_func(X_train, y_train)  # f(x) = loss_func(x_data, t_data)\n",
    "\n",
    "print(\"Initial error value = \", error_val(X_train, y_train), \"Initial W = \", W, \"\\n\", \", b = \", b )\n",
    "\n",
    "for step in  range(10001):  \n",
    "    \n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    \n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 400 == 0):\n",
    "        print(\"step = \", step, \"error value = \", error_val(X_train, y_train), \"W = \", W, \", b = \",b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. class로 구현한 LogisticRegression_class\n",
    "\n",
    "### 1을 참고하여 만든 모듈을 import하고 학습시켜주세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression_class.py 파일입니다!\n",
    "import numpy as np\n",
    "\n",
    "#시그모이드 함수\n",
    "\n",
    "def sigmoid(x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "\n",
    "#편미분 함수\n",
    "\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "\n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "\n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "\n",
    "    return grad\n",
    "\n",
    "#========================================================================================#\n",
    "\n",
    "class LogisticRegression_cls:\n",
    "\n",
    "    def __init__(self, X_train, y_train, learning_rate = 1e-2): #learning_rate를 기본값으로 준다!\n",
    "    \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.learning_rate = learning_rate\n",
    "        self.W = np.random.rand(1,1)  # W와 b는 class안에서만 사용되는 랜덤값이기때문에 이렇게 적어준다.\n",
    "        self.b = np.random.rand(1)  \n",
    "        \n",
    "        \n",
    "        \n",
    "    #손실함수\n",
    "    def loss_func(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "        z = np.dot(self.X_train,self.W) + self.b\n",
    "        y = sigmoid(z)\n",
    "    \n",
    "        # cross-entropy \n",
    "        return  -np.sum(self.y_train*np.log(y + delta) + (1-self.y_train)*np.log((1 - y)+delta ) )\n",
    "        \n",
    "        \n",
    "        \n",
    "    #손실 값 계산 함수\n",
    "    def error_val(self):\n",
    "        return self.loss_func() #손실함수인 loss_func에 계산까지 다 들어있기때문에 loss_func을 가져와 return해준다.\n",
    "    \n",
    "             \n",
    "    #예측 함수\n",
    "    def predict(self, X_test): #X_test는 class에 기본인수로 들어가는 것이 아니기때문에 self를 붙여주지않아도 된다. \n",
    "        result=[]\n",
    "        for x in X_test:\n",
    "            z=np.dot(x, self.W) + self.b\n",
    "            y=sigmoid(z)\n",
    "\n",
    "            if y > 0.5:\n",
    "                result.append(1)\n",
    "            else:\n",
    "                result.append(0)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    #학습 함수\n",
    "    def train(self):\n",
    "        f = lambda x : self.loss_func()  # f(x) = loss_func(x_data, t_data)\n",
    "\n",
    "        print(\"Initial error value = \", self.error_val())\n",
    "\n",
    "        for step in  range(10001):  \n",
    "            \n",
    "            self.W -= self.learning_rate * numerical_derivative(f, self.W)\n",
    "    \n",
    "            self.b -= self.learning_rate * numerical_derivative(f, self.b)\n",
    "    \n",
    "            if (step % 400 == 0):\n",
    "                print(\"step = \", step, \"error value = \", self.error_val())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LogisticRegression_class import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  16.075104375092234\n",
      "step =  0 error value =  7.543478269675374\n",
      "step =  400 error value =  2.6279170702445773\n",
      "step =  800 error value =  1.7565454011813404\n",
      "step =  1200 error value =  1.5021892728437451\n",
      "step =  1600 error value =  1.3418003014041993\n",
      "step =  2000 error value =  1.2281232501488022\n",
      "step =  2400 error value =  1.141688296683317\n",
      "step =  2800 error value =  1.0728133154471906\n",
      "step =  3200 error value =  1.016063708348713\n",
      "step =  3600 error value =  0.9681186142304529\n",
      "step =  4000 error value =  0.9268167663660799\n",
      "step =  4400 error value =  0.8906813844573943\n",
      "step =  4800 error value =  0.8586634868344296\n",
      "step =  5200 error value =  0.8299940165716682\n",
      "step =  5600 error value =  0.8040941162764901\n",
      "step =  6000 error value =  0.7805183009040323\n",
      "step =  6400 error value =  0.758917145674615\n",
      "step =  6800 error value =  0.7390120227219329\n",
      "step =  7200 error value =  0.7205775358273067\n",
      "step =  7600 error value =  0.7034290216267899\n",
      "step =  8000 error value =  0.687413472965227\n",
      "step =  8400 error value =  0.6724028272841598\n",
      "step =  8800 error value =  0.658288923100757\n",
      "step =  9200 error value =  0.6449796546568111\n",
      "step =  9600 error value =  0.6323960014422809\n",
      "step =  10000 error value =  0.6204697060936337\n"
     ]
    }
   ],
   "source": [
    "model_class = LogisticRegression_cls(X_train, y_train)\n",
    "model_class.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_class.predict(X_test)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial error value= 9.764485842269702\n",
      "step= 0  error value= 12.863813142431257\n",
      "step= 400  error value= 2.846142249503101\n",
      "step= 800  error value= 1.788197607309589\n",
      "step= 1200  error value= 1.5204882308809238\n",
      "step= 1600  error value= 1.3541484590332817\n",
      "step= 2000  error value= 1.2372216736883672\n",
      "step= 2400  error value= 1.1487820061829852\n",
      "step= 2800  error value= 1.0785656006766844\n",
      "step= 3200  error value= 1.020864695642025\n",
      "step= 3600  error value= 0.9722149043393165\n",
      "step= 4000  error value= 0.93037302454704\n",
      "step= 4400  error value= 0.893812414806018\n",
      "step= 4800  error value= 0.861452170808396\n",
      "step= 5200  error value= 0.8325019353976557\n",
      "step= 5600  error value= 0.8063681425340465\n",
      "step= 6000  error value= 0.7825948518906034\n",
      "step= 6400  error value= 0.7608250204761637\n",
      "step= 6800  error value= 0.7407743492387093\n",
      "step= 7200  error value= 0.7222131404261993\n",
      "step= 7600  error value= 0.7049534137459633\n",
      "step= 8000  error value= 0.6888395666098783\n",
      "step= 8400  error value= 0.6737414788181381\n",
      "step= 8800  error value= 0.6595493382828068\n",
      "step= 9200  error value= 0.6461697009864329\n",
      "step= 9600  error value= 0.6335224508539228\n",
      "step= 10000  error value= 0.6215384256864239\n"
     ]
    }
   ],
   "source": [
    "#정답\n",
    "#model_class = LogisticRegression_cls(X_train, y_train)\n",
    "#model_class.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#정답\n",
    "#y_pred = model_class.predict(X_test)\n",
    "\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. sklearn.linear_model의 LogisticRegression과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cute\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_sk = LogisticRegression()\n",
    "model_sk.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred_sk = model_sk.predict(X_test) \n",
    "\n",
    "print(y_pred_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LogisticRegression_cls와 sklearn의 LogisticRegression 둘다 똑같이 예측하고있는 것을 볼 수 있다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
